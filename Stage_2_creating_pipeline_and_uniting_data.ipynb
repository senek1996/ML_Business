{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Создание пайплайна для обработки признаков\n",
    "\n",
    "Принцип: берем список отобранных ранее полей из features, загружаем датафреймы, считываем данные из файла features.csv порциями по 100 тыс. строк, пропускаем полученные DataFrame'-ы через pipeline (заполнение пропусков), делаем left join через pd.DataFrame.merge и удаляем null-строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, luigi, dill\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from copy import deepcopy\n",
    "\n",
    "#создаем класс выбора столбцов pipeline\n",
    "class ColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "\n",
    "        try:\n",
    "            return X[self.columns]\n",
    "        except KeyError:\n",
    "            cols_error = list(set(self.columns) - set(X.columns))\n",
    "            raise KeyError(\"DataFrame не содердит следующие колонки: %s\" % cols_error)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем поля\n",
    "with open('cls.dat','rb') as f:\n",
    "    f_ok = dill.load(f)\n",
    "\n",
    "f_binary = f_ok[0]\n",
    "f_categorical = f_ok[1]\n",
    "f_numeric = f_ok[2]\n",
    "f_ok = f_ok[0] + f_ok[1] + f_ok[2]\n",
    "\n",
    "#Извлечем данные о пользователях\n",
    "data_train = pd.read_csv('data_train.csv')\n",
    "data_test = pd.read_csv('data_test.csv')\n",
    "\n",
    "#Первый столбец в датафреймах - просто номер строки. Уберем его:\n",
    "data_train = data_train[data_train.columns[1:]]\n",
    "data_test = data_test[data_test.columns[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все как на прошлом шаге.\n",
    "\n",
    "ВНИМАНИЕ: SimpleImputer ТРЕБУЕТ ПО КРАЙНЕЙ МЕРЕ ОДНОГО МАССИВА НА ВХОДЕ. А ИНАЧЕ ОШИБКА\n",
    "ValueError: at least one array or dtype is required\n",
    "\n",
    "Поэтому, поскольку у нас нет категориальных признаков, мы не добавляем соответствующий трансформер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('columnselector',\n",
       "  ColumnSelector(columns=['252', '25', '80', '14', '50', '27', '158', '178',\n",
       "                          '183', '175', '1', '141', '74', '219', '124', '133',\n",
       "                          '151', '37', '68', '16', '60', '251', '47', '97', '46',\n",
       "                          '174', '91', '32', '109', '132', ...])),\n",
       " ('featureunion',\n",
       "  FeatureUnion(transformer_list=[('numeric_features',\n",
       "                                  Pipeline(steps=[('columnselector',\n",
       "                                                   ColumnSelector(columns=['25',\n",
       "                                                                           '80',\n",
       "                                                                           '14',\n",
       "                                                                           '50',\n",
       "                                                                           '27',\n",
       "                                                                           '158',\n",
       "                                                                           '178',\n",
       "                                                                           '183',\n",
       "                                                                           '175',\n",
       "                                                                           '1',\n",
       "                                                                           '141',\n",
       "                                                                           '74',\n",
       "                                                                           '219',\n",
       "                                                                           '124',\n",
       "                                                                           '133',\n",
       "                                                                           '151',\n",
       "                                                                           '37',\n",
       "                                                                           '68',\n",
       "                                                                           '16',\n",
       "                                                                           '60',\n",
       "                                                                           '251',\n",
       "                                                                           '47',\n",
       "                                                                           '97',\n",
       "                                                                           '46',\n",
       "                                                                           '174',\n",
       "                                                                           '91',\n",
       "                                                                           '32',\n",
       "                                                                           '109',\n",
       "                                                                           '132',\n",
       "                                                                           '137', ...])),\n",
       "                                                  ('simpleimputer',\n",
       "                                                   SimpleImputer()),\n",
       "                                                  ('standardscaler',\n",
       "                                                   StandardScaler())])),\n",
       "                                 ('boolean_features',\n",
       "                                  Pipeline(steps=[('columnselector',\n",
       "                                                   ColumnSelector(columns=['252']))]))]))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#создаем pipeline\n",
    "f_prep_pipeline = make_pipeline(\n",
    "    ColumnSelector(columns=f_ok),\n",
    "    FeatureUnion(transformer_list=[\n",
    "        (\"numeric_features\", make_pipeline(\n",
    "            ColumnSelector(f_numeric),\n",
    "            SimpleImputer(strategy=\"mean\"),\n",
    "            StandardScaler()\n",
    "        )),\n",
    "        #(\"categorical_features\", make_pipeline(\n",
    "        #    ColumnSelector(f_categorical),\n",
    "        #    SimpleImputer(strategy=\"most_frequent\"),\n",
    "        #    OneHotEncoder(handle_unknown='ignore')\n",
    "        #)),\n",
    "        (\"boolean_features\", make_pipeline(\n",
    "            ColumnSelector(f_binary),\n",
    "        ))\n",
    "    ])\n",
    "    #ConcatenatorClear(columns=['id','buy_time'],X1 = data_train)\n",
    ")\n",
    "f_prep_pipeline.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass ConcatenatorClear(BaseEstimator, TransformerMixin):\\n    \\n    # Выполняет left join двух датафреймов\\n    # Датафреймы должны содержать обязательные колонки, по которым\\n    # будет производиться left join\\n    \\n    def __init__(self, columns, X1):\\n        self.columns = columns\\n        \\n        assert isinstance(X1, pd.DataFrame)\\n        \\n        if len(set(self.columns) - set(X1))==0:\\n            self.X1 = X1\\n        else:\\n            raise KeyError(\"ConcatenatorClear.init(): X1 содержитне все колонки из списка: %s\" % self.columns)\\n\\n    def fit(self, X, y=None):\\n        return self\\n    \\n    def transform(self, X2):\\n        assert isinstance(X2, pd.DataFrame) #проблема: X2 почему-то приходит типа sparse matrix вместо pd.DataFrame\\n        \\n        try:\\n            return self.X1.merge(X2, how=\\'left\\', on=self.columns)\\n        except:\\n            raise KeyError(\"Второй DataFrame не содержит необходимые поля: %s\" % self.columns)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Черновик\n",
    "'''\n",
    "class ConcatenatorClear(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Выполняет left join двух датафреймов\n",
    "    # Датафреймы должны содержать обязательные колонки, по которым\n",
    "    # будет производиться left join\n",
    "    \n",
    "    def __init__(self, columns, X1):\n",
    "        self.columns = columns\n",
    "        \n",
    "        assert isinstance(X1, pd.DataFrame)\n",
    "        \n",
    "        if len(set(self.columns) - set(X1))==0:\n",
    "            self.X1 = X1\n",
    "        else:\n",
    "            raise KeyError(\"ConcatenatorClear.init(): X1 содержитне все колонки из списка: %s\" % self.columns)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X2):\n",
    "        assert isinstance(X2, pd.DataFrame) #проблема: X2 почему-то приходит типа sparse matrix вместо pd.DataFrame\n",
    "        \n",
    "        try:\n",
    "            return self.X1.merge(X2, how='left', on=self.columns)\n",
    "        except:\n",
    "            raise KeyError(\"Второй DataFrame не содержит необходимые поля: %s\" % self.columns)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настроим параметры цикла создания объединенных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "portion_n = 1           #начальная порция\n",
    "portion_end = 100       #конечная порция\n",
    "portion_strs = 100000   #число строк в порции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим цикл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion 1... ended\n",
      "Portion 2... ended\n",
      "Portion 3... ended\n",
      "Portion 4... ended\n",
      "Portion 5... ended\n",
      "Portion 6... ended\n",
      "Portion 7... ended\n",
      "Portion 8... ended\n",
      "Portion 9... ended\n",
      "Portion 10... ended\n",
      "Portion 11... ended\n",
      "Portion 12... ended\n",
      "Portion 13... ended\n",
      "Portion 14... ended\n",
      "Portion 15... ended\n",
      "Portion 16... ended\n",
      "Portion 17... ended\n",
      "Portion 18... ended\n",
      "Portion 19... ended\n",
      "Portion 20... ended\n",
      "Portion 21... ended\n",
      "Portion 22... ended\n",
      "Portion 23... ended\n",
      "Portion 24... ended\n",
      "Portion 25... ended\n",
      "Portion 26... ended\n",
      "Portion 27... ended\n",
      "Portion 28... ended\n",
      "Portion 29... ended\n",
      "Portion 30... ended\n",
      "Portion 31... ended\n",
      "Portion 32... ended\n",
      "Portion 33... ended\n",
      "Portion 34... ended\n",
      "Portion 35... ended\n",
      "Portion 36... ended\n",
      "Portion 37... ended\n",
      "Portion 38... ended\n",
      "Portion 39... ended\n",
      "Portion 40... ended\n",
      "Portion 41... ended\n",
      "Portion 42... ended\n",
      "Portion 43... ended\n",
      "Portion 44... ended\n",
      "Portion 45... ended\n",
      "Portion 46... ended\n",
      "Portion 47... Данные закончились\n"
     ]
    }
   ],
   "source": [
    "for i in range(portion_n,portion_end):\n",
    "\n",
    "    print('Portion {}... '.format(i),end='')\n",
    "    \n",
    "    #Теперь извлечем признаки пользователей (первые 100000 строк):\n",
    "    if i==1:    \n",
    "        features = pd.read_csv('features.csv',nrows=portion_strs,delimiter='\\t')\n",
    "        features_columns = list(features.columns) #названия столбцов таблицы из исходного файла\n",
    "    else:\n",
    "        features = pd.read_csv('features.csv', nrows=portion_strs, skiprows=(i-1)*portion_strs+1, delimiter='\\t', header=0, names=features_columns)\n",
    "    \n",
    "    features = features[features.columns[1:]]\n",
    "    \n",
    "    #отдельно обработаем признак с номером 252: он бинарный, но иногда попадаются\n",
    "    #выбросы. Из-за этого кол-во столбцов после pipeline получается не всегда одним и тем же. Удалим соответствующие строки\n",
    "    ind_incor = features.loc[((features['252']!=0) & (features['252']!=1))].index\n",
    "    features.drop(ind_incor,inplace=True)\n",
    "    \n",
    "    features_left = features[['id','buy_time']] #левая часть features\n",
    "    \n",
    "    try:\n",
    "        features = f_prep_pipeline.fit_transform(features)\n",
    "    except:\n",
    "        print('Данные закончились')\n",
    "        break\n",
    "    \n",
    "    n_features = features.shape[1]\n",
    "    n_features\n",
    "    \n",
    "    #преобразование sparse-матрицы в DataFrame:\n",
    "    #features = features.todense()\n",
    "    features = pd.DataFrame(features)\n",
    "    features = pd.concat((features_left,features),axis=1,ignore_index=True)\n",
    "    \n",
    "    #делаем left join таблиц data_train и features2\n",
    "    features.columns = ['id','buy_time'] + list(features.columns)[:-2]\n",
    "    data_train2 = data_train.merge(features, how='left', on=['id','buy_time'])\n",
    "    data_test2 = data_test.merge(features, how='left', on=['id','buy_time'])\n",
    "    \n",
    "    #Ранее мы определили, что в data_train пропусков нет. Поэтому сейчас для большинства\n",
    "    #строк нет признаков. Поэтому удалим все строки, которые содержат nan-значения:\n",
    "    data_train2.dropna(inplace=True)\n",
    "    data_test2.dropna(inplace=True)\n",
    "        \n",
    "    if i==1:\n",
    "        #data_train_all = deepcopy(data_train2)\n",
    "        data_train2.to_csv('data_train_all.csv',mode='w',header=True,index=False)\n",
    "        data_test2.to_csv('data_test_all.csv',mode='w',header=True,index=False)\n",
    "    else:\n",
    "        #data_train_all = pd.concat((data_train_all,data_train2),axis=0,ignore_index=True)\n",
    "        data_train2.to_csv('data_train_all.csv',mode='a',header=False,index=False)\n",
    "        data_test2.to_csv('data_test_all.csv',mode='a',header=False,index=False)\n",
    "    \n",
    "    print('ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, мы ошиблись с размером данных - вместо 11 млн. строк в файле features.csv их кол-во составляет чуть более 4,6 млн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
